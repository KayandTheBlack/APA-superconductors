---
title: "NonlinearMethods"
author: "Victor G."
date: "2 de enero de 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
LE <- read.csv("learn_set.csv", na.strings = c("?"))
LE <- LE[,-1] #removal of indices
```


```{r}
library(caret)
#trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
trctrl <- trainControl(method = "none")

(decays <- 10^seq(-3,0,by=0.1))
df <- data.frame(Decay=double(),
                 Error=double(),
                 stringsAsFactors=FALSE)

set.seed(42)
LE.optim_ind <- sample(seq_len(nrow(LE)), size = floor(0.2*nrow(LE)))
LE.optim <- LE[LE.optim_ind,]
VAL.optim <- LE[-LE.optim_ind,]

```

```{r}
LE.optim.x <- LE.optim[, names(LE.optim) != "critical_temp"] 
LE.optim.y <- LE.optim[, "critical_temp"] 
```


First we'll fix a large number of hidden units in one hidden layer, and explore different regularization values.
(note: this will take a bit to execute)

```{r}

print(c("decay","error"))
start_time <- Sys.time()
last <- Sys.time()
for (dec in decays){
  model.mlp <- train (y = LE.optim.y, x=LE.optim.x,
                        method='nnet', 
                        maxit = 500, 
                        trace = FALSE,
                        linout = T,
                        tuneGrid = expand.grid(.size=40,.decay=dec), 
                        trControl=trctrl,
                        MaxNWts = 5000
                      )
  
  pred <- predict (model.mlp, VAL.optim[,-which(names(VAL.optim) == "critical_temp")])
  error <- sum((pred-VAL.optim[,"critical_temp"])^2)/((length(pred)-1)*var(LE[,"critical_temp"]))
  
  #squaredError = sum(abs(p2 - rock.y[learn])^2)
  print(c(dec,error))
  # postResample(p2,VAL.optim[,"critical_temp"])
  
  newRow <- data.frame(Decay=dec,Error=error)
  df <- rbind(df,newRow)
  curr <- Sys.time()
  
  print(curr-last)
  last <- Sys.time()
}

end_time <- Sys.time()
end_time - start_time

df
plot(df)


```

We can observe the best results with a regularization value of 1.

Now we'll explore different numbers of hidden units in one hidden layer with that regularization value.
(note: this will take a bit to execute)

```{r}

(sizes <- seq(1,40,by=1))

df2 <- data.frame(Size=integer(),
                 Error=double(),
                 stringsAsFactors=FALSE)

print(c("decay","error"))
start_time <- Sys.time()
last <- Sys.time()
for (siz in sizes){
  model.mlp <- train (y = LE.optim.y, x=LE.optim.x,
                        method='nnet', 
                        maxit = 500, 
                        trace = FALSE,
                        linout = T,
                        tuneGrid = expand.grid(.size=siz,.decay=1.0), 
                        trControl=trctrl,
                        MaxNWts = 5000
                      )
  
  pred <- predict (model.mlp, VAL.optim[,-which(names(VAL.optim) == "critical_temp")])
  error <- sum((pred-VAL.optim[,"critical_temp"])^2)/((length(pred)-1)*var(LE[,"critical_temp"]))
  
  print(c(siz,error))
  # postResample(p2,VAL.optim[,"critical_temp"])
  
  newRow <- data.frame(Size=siz,Error=error)
  df2 <- rbind(df2,newRow)
  curr <- Sys.time()
  
  print(curr-last)
  last <- Sys.time()
}

end_time <- Sys.time()
end_time - start_time

df2
plot(df2)

```

In this case it seems we'll get best results using 11 or 18. Since they are very similar, we'll go with 11 since it's simpler.

Now we'll calculate the error using repeated-cross-validation.
  
```{r}
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
print(c("size","decay","error"))

model.mlp <- train (y = LE.optim.y, x=LE.optim.x,
                      method='nnet', 
                      maxit = 500, 
                      trace = FALSE,
                      linout = T,
                      tuneGrid = expand.grid(.size=11,.decay=1.0), 
                      trControl=trctrl,
                      MaxNWts = 5000
                    )

pred <- predict (model.mlp, VAL.optim[,-which(names(VAL.optim) == "critical_temp")])
error <- sum((pred-VAL.optim[,"critical_temp"])^2)/((length(pred)-1)*var(LE[,"critical_temp"]))

LE.x <- LE[, names(LE) != "critical_temp"] 
LE.y <- LE[, "critical_temp"] 

print(c(11,1.0,error))




```

We end up with an alright 18.8% of error.


SVM with RBF kernel

We will follow the same approach as with quadratic SVM in linear methods, but this time we must also optimize the sigma value of the Kernel. We should do both at the same time, but it is very costly in execution time, so we will do a sort of 'hillclimbing', optimizing one then the other succesively.
```{r}
library(kernlab)
```

```{r}
set.seed(42)
LE.optim_ind <- sample(seq_len(nrow(LE)), size = floor(0.2*nrow(LE)))
LE.optim <- LE[LE.optim_ind,]
VAL.optim <- LE[-LE.optim_ind,]

ptm <- proc.time()
exps <- seq(-3,3,0.5)
Cs <- 10^exps
error <- rep(0,length(Cs))
for (i in 1:length(Cs)) {
  print(i)
  svm.opt = ksvm(critical_temp ~ ., data = LE.optim, C=Cs[i], kernel = rbfdot())
  pred <- predict(svm.opt, VAL.optim[,-which(names(VAL.optim) == "critical_temp")])
  error[i] <- sum((pred-VAL.optim[,"critical_temp"])^2)/((length(pred)-1)*var(LE[,"critical_temp"]))
}
(proc.time() - ptm)
plot(exps, error)
```


```{r}
LE.optim_ind <- sample(seq_len(nrow(LE)), size = floor(0.2*nrow(LE)))
LE.optim <- LE[LE.optim_ind,]
VAL.optim <- LE[-LE.optim_ind,]

ptm <- proc.time()
exps <- seq(0,2,0.25)
Cs <- 10^exps
error <- rep(0,length(Cs))
for (i in 1:length(Cs)) {
  print(i)
  svm.opt = ksvm(critical_temp ~ ., data = LE.optim, C=Cs[i], kernel = rbfdot())
  pred <- predict(svm.opt, VAL.optim[,-which(names(VAL.optim) == "critical_temp")])
  error[i] <- sum((pred-VAL.optim[,"critical_temp"])^2)/((length(pred)-1)*var(LE[,"critical_temp"]))
}
(proc.time() - ptm)
plot(exps, error)
```

So first C will be 10^0.5

```{r}
LE.optim_ind <- sample(seq_len(nrow(LE)), size = floor(0.2*nrow(LE)))
LE.optim <- LE[LE.optim_ind,]
VAL.optim <- LE[-LE.optim_ind,]

ptm <- proc.time()
exps <- seq(-2,2,0.5)
Ys <- 10^exps
error <- rep(0,length(Cs))
for (i in 1:length(Cs)) {
  print(i)
  svm.opt = ksvm(critical_temp ~ ., data = LE.optim, C=10^0.5, kernel = rbfdot(sigma=Ys[i]))
  pred <- predict(svm.opt, VAL.optim[,-which(names(VAL.optim) == "critical_temp")])
  error[i] <- sum((pred-VAL.optim[,"critical_temp"])^2)/((length(pred)-1)*var(LE[,"critical_temp"]))
}
(proc.time() - ptm)
plot(exps, error)
```

```{r}
LE.optim_ind <- sample(seq_len(nrow(LE)), size = floor(0.2*nrow(LE)))
LE.optim <- LE[LE.optim_ind,]
VAL.optim <- LE[-LE.optim_ind,]

ptm <- proc.time()
exps <- seq(-3,-1,0.25)
Ys <- 10^exps
error <- rep(0,length(Cs))
for (i in 1:length(Cs)) {
  print(i)
  svm.opt = ksvm(critical_temp ~ ., data = LE.optim, C=10^0.5, kernel = rbfdot(sigma=Ys[i]))
  pred <- predict(svm.opt, VAL.optim[,-which(names(VAL.optim) == "critical_temp")])
  error[i] <- sum((pred-VAL.optim[,"critical_temp"])^2)/((length(pred)-1)*var(LE[,"critical_temp"]))
}
(proc.time() - ptm)
plot(exps, error)
```




We optimize C again, this time with sigma = 10^-1.25






```{r}
LE.optim_ind <- sample(seq_len(nrow(LE)), size = floor(0.2*nrow(LE)))
LE.optim <- LE[LE.optim_ind,]
VAL.optim <- LE[-LE.optim_ind,]

ptm <- proc.time()
exps <- seq(0.5,1.5,0.1)
Cs <- 10^exps
error <- rep(0,length(Cs))
for (i in 1:length(Cs)) {
  print(i)
  svm.opt = ksvm(critical_temp ~ ., data = LE.optim, C=Cs[i], kernel = rbfdot(sigma = 10^(-1.25)))
  pred <- predict(svm.opt, VAL.optim[,-which(names(VAL.optim) == "critical_temp")])
  error[i] <- sum((pred-VAL.optim[,"critical_temp"])^2)/((length(pred)-1)*var(LE[,"critical_temp"]))
}
(proc.time() - ptm)
plot(exps, error)
```


We can observe the error goes down from .170 to .167 aproximately by optimizing the C parameter again. Since this improvement is not too big, we will leave it here, and run the crossvalidation

```{r}
k<-10
N<-nrow(LE)
folds <- sample(rep(1:k, length=N), N, replace=FALSE) 
error <- rep(0,k)

for (i in 1:k) {
  print(i)
  train <- LE[folds!=i,] # for building the model (training)
  valid <- LE[folds==i,] # for prediction (validation)
  finalRBFSVM <- ksvm(critical_temp ~ ., data = train, C=10^1.1, kernel = rbfdot(sigma = 10^(-1.25)))
  
  pred <- predict(finalRBFSVM, valid[,-which(names(valid) == "critical_temp")])
  error[i] <- sum((pred-valid[,"critical_temp"])^2)/((length(pred)-1)*var(LE[,"critical_temp"]))
}
(100*sum(error)/k)
```

Our error is a decent 11.5%. Studying one of the SVMs trained, we find that the number of support vectors is 6896 out of 13396 members in the training set. This means that around 50% of the training data is inside the epsilon zone of 0.1, or less than 0.1 away from the predicted value. Our training error is 8% too.

We note that we are not using the Root MSE nor the MSE due to the fact that we scaled the critical temperature. We will be able to revert this normalisation to obtain the MSE by multiplying by the original variance (34.25436^2). This is because the real MSE is actually the current MSE but with the internal values divided by the standard deviation (from the scale function back in preprocessing), and since the difference is squared we should also square the sd. So, in relation with our MSE on the normalized critical_temperature, the real MSE is sd^2 times greater. Note that since both elements in the difference have the mean, we do not take it into account here, but to predict real values we must also add the original mean afterwards.

For example, with our case of 11.5% NMSE, we'd have:

```{r}
MSE.standard <- 11.50602/100*(1487*var(LE[,"critical_temp"]))/1488
(RMSE.real <- MSE.standard^0.5*34.25436)
```

This would mean we can expect 11.58 K of difference between the predicted and the actual result.

But for now NMSE is enough for our purposes.




















